{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6099b6ef",
      "metadata": {
        "id": "6099b6ef",
        "papermill": {
          "duration": 0.013715,
          "end_time": "2025-05-12T07:36:54.250640",
          "exception": false,
          "start_time": "2025-05-12T07:36:54.236925",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Install required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e986b62",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T07:36:54.277134Z",
          "iopub.status.busy": "2025-05-12T07:36:54.276824Z",
          "iopub.status.idle": "2025-05-12T07:39:16.682679Z",
          "shell.execute_reply": "2025-05-12T07:39:16.681410Z"
        },
        "id": "5e986b62",
        "papermill": {
          "duration": 142.421789,
          "end_time": "2025-05-12T07:39:16.685025",
          "exception": false,
          "start_time": "2025-05-12T07:36:54.263236",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df0dc87-6563-4ead-bff5-a8781890d320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install gradio --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install huggingface_hub --quiet\n",
        "!pip install huggingface_hub[hf_xet] --quiet\n",
        "!pip install \"datasets>=3.4.1\" --quiet\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515818de",
      "metadata": {
        "id": "515818de",
        "papermill": {
          "duration": 0.04077,
          "end_time": "2025-05-12T07:39:16.767534",
          "exception": false,
          "start_time": "2025-05-12T07:39:16.726764",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55ecbc4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T07:39:16.853085Z",
          "iopub.status.busy": "2025-05-12T07:39:16.852688Z",
          "iopub.status.idle": "2025-05-12T07:39:59.650923Z",
          "shell.execute_reply": "2025-05-12T07:39:59.649921Z"
        },
        "id": "c55ecbc4",
        "papermill": {
          "duration": 42.844545,
          "end_time": "2025-05-12T07:39:59.652791",
          "exception": false,
          "start_time": "2025-05-12T07:39:16.808246",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1906f89",
      "metadata": {
        "id": "a1906f89",
        "papermill": {
          "duration": 0.154403,
          "end_time": "2025-05-12T07:39:59.848659",
          "exception": false,
          "start_time": "2025-05-12T07:39:59.694256",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Importing HF Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d3ad76",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T07:39:59.931764Z",
          "iopub.status.busy": "2025-05-12T07:39:59.930935Z",
          "iopub.status.idle": "2025-05-12T07:40:10.096657Z",
          "shell.execute_reply": "2025-05-12T07:40:10.095086Z"
        },
        "id": "e4d3ad76",
        "papermill": {
          "duration": 10.209235,
          "end_time": "2025-05-12T07:40:10.098461",
          "exception": true,
          "start_time": "2025-05-12T07:39:59.889226",
          "status": "failed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "huggingface_user=userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff262124",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff262124",
        "outputId": "09a2b134-ef94-4ad9-ed9c-083511283d69",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'type': 'user', 'id': '66957888f4d5f5d06ca36462', 'name': 'Suman2004', 'fullname': 'Suman Roy', 'email': 'sumanroy202400@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': 1748735999, 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66957888f4d5f5d06ca36462/BQzGRCRMfy4WL6NSsAICJ.jpeg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'sent_token', 'role': 'write', 'createdAt': '2025-05-14T17:59:22.556Z'}}}\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import whoami\n",
        "print(whoami(token=huggingface_user))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=huggingface_user)"
      ],
      "metadata": {
        "id": "IOb8j-bsnTbc"
      },
      "id": "IOb8j-bsnTbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7f39fb4",
      "metadata": {
        "id": "b7f39fb4",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "# Loading the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText, TextStreamer\n",
        "import torch\n",
        "\n",
        "# Load your fine-tuned model and processor\n",
        "model_id = \"Suman2004/covid_chestXray_radiologist\"\n",
        "processor = AutoProcessor.from_pretrained(model_id,token=huggingface_user)\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id,token=huggingface_user, torch_dtype=torch.float16).to(\"cuda\")"
      ],
      "metadata": {
        "id": "sGK6BrZajlq6"
      },
      "id": "sGK6BrZajlq6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launching the gradio app"
      ],
      "metadata": {
        "id": "s1VNQ7oXJSuG"
      },
      "id": "s1VNQ7oXJSuG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction on Single"
      ],
      "metadata": {
        "id": "egF4WL2FKwrm"
      },
      "id": "egF4WL2FKwrm"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Load image (same format as during training)\n",
        "image = Image.open(\"/content/Normal.jpg\")\n",
        "\n",
        "# Instruction text (same as during fine-tuning)\n",
        "instruction = '''You are an expert radiologist. Analyze the provided chest X-ray image and classify the condition into:\n",
        "\n",
        "- COVID-19\n",
        "- Normal\n",
        "- Pneumonia (non-COVID)\n",
        "\n",
        "Respond with:\n",
        "1. Classification: [Condition Name]\n",
        "2. Justification: [Clinical Findings]'''\n",
        "\n",
        "# Format as chat-style messages (same structure as training)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": instruction},\n",
        "            {\"type\": \"image\", \"image\": image}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Apply chat template to get proper prompt format\n",
        "prompt = processor.tokenizer.apply_chat_template(messages, add_generation_prompt=True,tokenize=False)\n",
        "\n",
        "# Preprocess with text + image\n",
        "inputs = processor(\n",
        "    text=prompt,\n",
        "    images=image,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\", torch.float16)\n",
        "\n",
        "# Stream output (optional but useful for long generation)\n",
        "streamer = TextStreamer(processor.tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate the model output\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=300,\n",
        "    use_cache=True,\n",
        "    temperature=1.2,\n",
        "    top_p=0.9\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik2dzeO6gqfH",
        "outputId": "919e6651-1ea6-4ffc-c8a5-641de73503a4"
      },
      "id": "Ik2dzeO6gqfH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Classification: Normal\n",
            "2. Justification: No focal consolidations or opacities observed. Lung fields appear clear.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRADIO APP"
      ],
      "metadata": {
        "id": "TWup2xvLLDJi"
      },
      "id": "TWup2xvLLDJi"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Define the instruction (same as during fine-tuning)\n",
        "instruction = '''You are an expert radiologist. Analyze the provided chest X-ray image and classify the condition into:\n",
        "\n",
        "- COVID-19\n",
        "- Normal\n",
        "- Pneumonia (non-COVID)\n",
        "\n",
        "Respond with:\n",
        "1. Classification: [Condition Name]\n",
        "2. Justification: [Clinical Findings]'''\n",
        "\n",
        "# Inference function\n",
        "def analyze_xray(image: Image.Image):\n",
        "    image = image.convert(\"RGB\")  # Ensure RGB\n",
        "\n",
        "    # Construct chat prompt\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "                {\"type\": \"image\", \"image\": image}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    prompt = processor.tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=False\n",
        "    )\n",
        "\n",
        "    # Preprocess\n",
        "    inputs = processor(\n",
        "        text=prompt,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\", torch.float16)\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            use_cache=True,\n",
        "            temperature=1.2,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    decoded = processor.tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "    assistant_part = decoded.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1]\n",
        "    result = assistant_part.split(\"<|eot_id|>\")[0].strip()\n",
        "    return result\n",
        "\n",
        "# Example images\n",
        "example_images = [\n",
        "    [\"/content/Normal.jpg\"],\n",
        "    [\"/content/Pneumonia.jpg\"],\n",
        "    [\"/content/covid.jpg\"]\n",
        "]\n",
        "\n",
        "# Create Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=analyze_xray,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload Chest X-ray\"),\n",
        "    outputs=gr.Textbox(label=\"Model Analysis\"),\n",
        "    title=\"Chest X-ray Diagnosis (AI Radiologist)\",\n",
        "    description=\"Upload a chest X-ray to classify it as COVID-19, Pneumonia (non-COVID), or Normal. The model provides justification based on visual findings.\",\n",
        "    examples=example_images\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "iface.launch(pwa=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "lRh09Rf9jl9J",
        "outputId": "d3bebda2-ad98-4343-80c1-55009bced238"
      },
      "id": "lRh09Rf9jl9J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ea2c3505bca01b4075.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ea2c3505bca01b4075.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 204.329255,
      "end_time": "2025-05-12T07:40:13.468112",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-05-12T07:36:49.138857",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}